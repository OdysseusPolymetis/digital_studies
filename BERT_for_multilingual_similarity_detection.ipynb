{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OdysseusPolymetis/digital_studies/blob/main/BERT_for_multilingual_similarity_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG7DZEpdzDLu"
      },
      "source": [
        "# <center>**Mesures de similarités entre le grec et les traductions latines avec Multilingual BERT**</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUP8_s_FuAH9"
      },
      "source": [
        "Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKrcW3eIdoZM"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers numpy stanza"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous entreposons nos pickles et notre modèle dans notre drive, vous pouvez changer les chemins."
      ],
      "metadata": {
        "id": "SQ1bh4voJeHh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNyToyX_zgKU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Téléchargement des données sur le dépôt Perseus"
      ],
      "metadata": {
        "id": "SGERNMXIJrSz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skftlq81zZlz"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/PerseusDL/canonical-greekLit.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwO2DZ8zpmZA"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/PerseusDL/canonical-latinLit.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlSKHcvvuVzD"
      },
      "source": [
        "Imports nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKXQ_J3RzphS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import glob\n",
        "from joblib import Parallel, delayed\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "from lxml import etree\n",
        "import stanza\n",
        "from tqdm import tqdm\n",
        "import unicodedata\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyKyGu5rfsqx"
      },
      "source": [
        "## Récupération et paramétrage du modèle affiné"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3Ck5s1-fr6-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "model_directory = \"/content/drive/MyDrive/MBERT_Models/finetuned_mbert_model_best\"\n",
        "\n",
        "model = BertModel.from_pretrained(model_directory)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_directory + \"/vocab.txt\")\n",
        "model.eval()  # Mode évaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONyuud1IkX3g"
      },
      "source": [
        "Paramétrage du modèle sur GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SSFlUoL-a-R"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraction des éléments XML"
      ],
      "metadata": {
        "id": "CVQfB7OuKJa6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aqa4Qgjb7Fzb"
      },
      "outputs": [],
      "source": [
        "def extract_body_content_from_xml(file_path):\n",
        "    parser = etree.XMLParser(recover=True)\n",
        "    tree = etree.parse(file_path, parser)\n",
        "    nsmap = tree.getroot().nsmap\n",
        "    default_ns = nsmap.get(None)\n",
        "\n",
        "    if default_ns:\n",
        "        body = tree.find(\".//ns:body\", namespaces={\"ns\": default_ns})\n",
        "    else:\n",
        "        body = tree.find(\".//body\")\n",
        "\n",
        "    if body is None:\n",
        "        raise ValueError(f\"No <body> element found in {file_path}\")\n",
        "\n",
        "    return etree.tostring(body, method=\"text\", encoding=\"unicode\")\n",
        "\n",
        "def is_latin(filename):\n",
        "    return re.search(r'lat\\d+\\.xml$', filename) is not None\n",
        "\n",
        "def is_greek(filename):\n",
        "    return re.search(r'grc\\d+\\.xml$', filename) is not None\n",
        "\n",
        "def extract_texts_from_directory(directory_path):\n",
        "    \"\"\"\n",
        "    Extrait les textes de chaque fichier XML dans le répertoire spécifié et les stocke dans un dictionnaire.\n",
        "    \"\"\"\n",
        "    texts = {}\n",
        "    for root, _, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if is_latin(file) or is_greek(file):\n",
        "                file_path = os.path.join(root, file)\n",
        "                content = extract_body_content_from_xml(file_path)\n",
        "                texts[file] = content\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv2QXLyhjscq"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_encode_texts(texts, tokenizer):\n",
        "    tokenized_texts = {}\n",
        "    for text_key, text_value in texts.items():\n",
        "        tokens = tokenizer.tokenize(text_value)\n",
        "        # Tronquer à la longueur maximale de 512\n",
        "        tokens = tokens[:512]\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        tokenized_texts[text_key] = {\n",
        "            'tokens': tokens,\n",
        "            'input_ids': input_ids\n",
        "        }\n",
        "    return tokenized_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjzlJ6jN61BR"
      },
      "outputs": [],
      "source": [
        "greek_corpus = extract_texts_from_directory('/content/canonical-greekLit')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1LV-ZIUjvaE"
      },
      "outputs": [],
      "source": [
        "latin_corpus = extract_texts_from_directory('/content/canonical-latinLit')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatisation avec Stanza\n",
        "<br>Sauvegardes en pickle"
      ],
      "metadata": {
        "id": "6bopiRR4KREF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHlKuAoJ6aHX"
      },
      "outputs": [],
      "source": [
        "stanza.download('grc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq_hKlS5j8hR"
      },
      "outputs": [],
      "source": [
        "stanza.download('la')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg3bCn1a7n4p"
      },
      "outputs": [],
      "source": [
        "def process_text(text, nlp):\n",
        "    \"\"\"\n",
        "    Traite le texte avec Stanza pour obtenir à la fois les formes fléchies et les lemmes.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    processed_text = {\n",
        "        \"lemmas\": [],\n",
        "        \"forms\": []\n",
        "    }\n",
        "\n",
        "    for sentence in doc.sentences:\n",
        "        lemmatized_sentence = []\n",
        "        forms_sentence = []\n",
        "        for word in sentence.words:\n",
        "            lemmatized_sentence.append(word.lemma)\n",
        "            forms_sentence.append(word.text)\n",
        "        processed_text[\"lemmas\"].append(lemmatized_sentence)\n",
        "        processed_text[\"forms\"].append(forms_sentence)\n",
        "\n",
        "    return processed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suT9P2GH7rDb"
      },
      "outputs": [],
      "source": [
        "nlp = stanza.Pipeline('grc', processors='tokenize,pos,lemma', use_gpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBD_RulukCrm"
      },
      "outputs": [],
      "source": [
        "nlp_latin = stanza.Pipeline('la', processors='tokenize,pos,lemma', use_gpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZCRN0DFLWLk-"
      },
      "outputs": [],
      "source": [
        "filenames = list(greek_corpus.keys())\n",
        "\n",
        "# 2. Effectuer la lemmatisation sur cette portion\n",
        "greek_corpus_processed = {}\n",
        "for filename in tqdm(filenames, desc=\"Processing\"):\n",
        "    greek_corpus_processed[filename] = process_text(greek_corpus[filename], nlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMHvvqwGkMft"
      },
      "outputs": [],
      "source": [
        "filenames = list(latin_corpus.keys())\n",
        "\n",
        "# 2. Effectuer la lemmatisation sur cette portion\n",
        "latin_corpus_processed = {}\n",
        "for filename in tqdm(filenames, desc=\"Processing\"):\n",
        "    latin_corpus_processed[filename] = process_text(latin_corpus[filename], nlp_latin)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sauvegarde"
      ],
      "metadata": {
        "id": "WInL0N3AKssv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkNuQLnsWlVJ"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/embeddings_save/lemmatized_greek_perseus.pkl', 'wb') as file:\n",
        "    pickle.dump(greek_corpus_processed, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Euo_u2T0xke"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/embeddings_save/greek_corpus_processed.pkl', 'rb') as file:\n",
        "    greek_corpus_processed = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcHukOaYkj68"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/embeddings_save/latin_corpus_processed.pkl', 'wb') as file:\n",
        "    pickle.dump(latin_corpus_processed, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/embeddings_save/latin_corpus_processed.pkl', 'rb') as file:\n",
        "    latin_corpus_processed = pickle.load(file)"
      ],
      "metadata": {
        "id": "ZbIZImgSxsqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérification des données"
      ],
      "metadata": {
        "id": "QwLvc5fcKuug"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgiUJ2Okgu86"
      },
      "outputs": [],
      "source": [
        "for key, value in list(latin_corpus_processed.items())[:5]:\n",
        "    lemmas = value[\"lemmas\"]\n",
        "    forms = value[\"forms\"]\n",
        "    print(f\"Filename: {key}\")\n",
        "    print(f\"Lemmatized Content: {' '.join(lemmas[0])[:200]}...\")  # Print only first 200 chars of the lemmatized content of the first sentence\n",
        "    print(f\"Forms Content: {' '.join(forms[0])[:200]}...\\n\")  # Print only first 200 chars of the forms content of the first sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redécoupage des phrases de plus de 510 tokens (BERT accepte 512 tokens par séquence)"
      ],
      "metadata": {
        "id": "1vdGl0eCK01T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZF2bxvN9oeF"
      },
      "outputs": [],
      "source": [
        "def split_long_sentences(corpus, tokenizer, max_length=510):\n",
        "    split_corpus = {\"forms\": {}, \"lemmas\": {}}\n",
        "\n",
        "    for filename, text in tqdm(corpus.items(), desc=\"Splitting long sentences\"):\n",
        "        split_forms, split_lemmas = [], []\n",
        "\n",
        "        for forms_sentence, lemmas_sentence in zip(text[\"forms\"], text[\"lemmas\"]):\n",
        "            tokenized_words = [tokenizer.tokenize(word) for word in forms_sentence]\n",
        "            token_lengths = [len(tokens) for tokens in tokenized_words]\n",
        "\n",
        "            sub_sentence_tokens, sub_forms, sub_lemmas = [], [], []\n",
        "            for word, lemma, tokens, token_length in zip(forms_sentence, lemmas_sentence, tokenized_words, token_lengths):\n",
        "                if len(sub_sentence_tokens) + token_length <= max_length:\n",
        "                    sub_forms.append(word)\n",
        "                    sub_lemmas.append(lemma)\n",
        "                    sub_sentence_tokens.extend(tokens)\n",
        "                else:\n",
        "                    split_forms.append(sub_forms)\n",
        "                    split_lemmas.append(sub_lemmas)\n",
        "                    sub_forms, sub_lemmas, sub_sentence_tokens = [word], [lemma], tokens\n",
        "            if sub_forms:\n",
        "                split_forms.append(sub_forms)\n",
        "                split_lemmas.append(sub_lemmas)\n",
        "\n",
        "        split_corpus[\"forms\"][filename] = split_forms\n",
        "        split_corpus[\"lemmas\"][filename] = split_lemmas\n",
        "\n",
        "    return split_corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3XKnN7cqzgN"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentence(sentence, tokenizer):\n",
        "    return tokenizer.tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIiVCRQE9ram"
      },
      "outputs": [],
      "source": [
        "split_greek_corpus = split_long_sentences(greek_corpus_processed, tokenizer, 510)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OJxsptNZwTh"
      },
      "outputs": [],
      "source": [
        "split_latin_corpus = split_long_sentences(latin_corpus_processed, tokenizer, 510)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sauvegarde du split"
      ],
      "metadata": {
        "id": "ezaZANR2K-1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/embeddings_save/split_latin_corpus.pkl', 'wb') as file:\n",
        "    pickle.dump(split_latin_corpus, file)\n",
        "with open('/content/drive/MyDrive/embeddings_save/split_greek_corpus.pkl', 'wb') as file:\n",
        "    pickle.dump(split_greek_corpus, file)"
      ],
      "metadata": {
        "id": "pOgC_w97DCa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/embeddings_save/split_latin_corpus.pkl', 'rb') as file:\n",
        "    split_latin_corpus = pickle.load(file)\n",
        "with open('/content/drive/MyDrive/embeddings_save/split_greek_corpus.pkl', 'rb') as file:\n",
        "    split_greek_corpus = pickle.load(file)"
      ],
      "metadata": {
        "id": "fz80zJBdnSMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérification"
      ],
      "metadata": {
        "id": "NjZOs7W7LEuv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMkIP53viBbr"
      },
      "outputs": [],
      "source": [
        "for key, value in list(split_greek_corpus[\"forms\"].items())[:5]:\n",
        "    print(f\"Filename: {key}\")\n",
        "    print(f\"Forms Content (first sentence): {' '.join(value[0])[:200]}...\\n\")  # Print only first 200 chars of the forms content of the first sentence\n",
        "\n",
        "print(\"\\n---\\n\")\n",
        "\n",
        "for key, value in list(split_greek_corpus[\"lemmas\"].items())[:5]:\n",
        "    print(f\"Filename: {key}\")\n",
        "    print(f\"Lemmatized Content (first sentence): {' '.join(value[0])[:200]}...\\n\")  # Print only first 200 chars of the lemmatized content of the first sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkrFJLFWbraw"
      },
      "outputs": [],
      "source": [
        "def verify_split_corpus(split_corpus, tokenizer, max_length=510):\n",
        "    # Ici, nous utilisons tqdm pour envelopper l'itération sur les fichiers\n",
        "    for filename in tqdm(split_corpus[\"forms\"].keys(), desc=\"Verifying files\"):\n",
        "        forms_sentences = split_corpus[\"forms\"][filename]\n",
        "        lemmas_sentences = split_corpus[\"lemmas\"][filename]\n",
        "\n",
        "        if len(forms_sentences) != len(lemmas_sentences):\n",
        "            print(f\"Error: Mismatched number of sentences for file {filename}\")\n",
        "            continue\n",
        "\n",
        "        for idx, (forms_sentence, lemmas_sentence) in enumerate(zip(forms_sentences, lemmas_sentences)):\n",
        "            tokenized_sentence = tokenizer.tokenize(\" \".join(forms_sentence))\n",
        "\n",
        "            if len(tokenized_sentence) > max_length:\n",
        "                print(f\"Error: Sentence {idx} in file {filename} exceeds {max_length} tokens.\")\n",
        "\n",
        "            if len(forms_sentence) != len(lemmas_sentence):\n",
        "                print(f\"Error: Mismatched number of words in sentence {idx} of file {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG1qho9ebz9u"
      },
      "outputs": [],
      "source": [
        "print(\"Starting verification...\")\n",
        "verify_split_corpus(split_greek_corpus, tokenizer)\n",
        "print(\"\\nVerification complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tri sur les auteurs"
      ],
      "metadata": {
        "id": "6Z8FTiJ9LJWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le tri des auteurs se fait sur les identifiants des auteurs présents dans le titre des fichiers."
      ],
      "metadata": {
        "id": "9yvEwEj9LNyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Endroit où stocker tous les auteurs disponibles\n",
        "grc_target_authors = [\"tlg0059\",\"tlg0086\",\"tlg1325\", \"tlg0626\",\"tlg1304\",\"tlg0632\",\"tlg0591\",\"tlg0593\",\"tlg1562\",\"tlg1705\",\"tlg0014\",\"tlg0610\"]\n",
        "lat_target_authors = [\"phi0474\", \"phi1017\",\"stoa0255\",\"phi1014\",\"tlg0557\",\"phi0550\",\"tlg0628\",\"tlg0562\",\"phi1254\",\"phi1002\",\"stoa0058\",\"phi0684\",\"phi1212\"]"
      ],
      "metadata": {
        "id": "KGCNx3c7nr3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Première tranche latine\n",
        "grc_target_authors = [\"tlg0059\",\"tlg0086\",\"tlg1325\", \"tlg0626\",\"tlg1304\",\"tlg0632\",\"tlg0591\",\"tlg0593\",\"tlg1562\",\"tlg1705\",\"tlg0014\",\"tlg0610\"]\n",
        "lat_target_authors = [\"phi0474\", \"phi1017\",\"stoa0255\",\"phi1014\",\"tlg0557\",\"phi0684\"]"
      ],
      "metadata": {
        "id": "t1go6CJPLZ_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seconde tranche latine\n",
        "grc_target_authors = [\"tlg0059\",\"tlg0086\",\"tlg1325\", \"tlg0626\",\"tlg1304\",\"tlg0632\",\"tlg0591\",\"tlg0593\",\"tlg1562\",\"tlg1705\",\"tlg0014\",\"tlg0610\"]\n",
        "lat_target_authors = [\"phi0550\", \"tlg0628\", \"tlg0562\", \"phi1254\", \"phi1002\", \"stoa0058\", \"phi1212\"]"
      ],
      "metadata": {
        "id": "c97A0bo9La6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_lat_corpus = {}\n",
        "filtered_grc_corpus = {}\n",
        "for categorie in split_latin_corpus:\n",
        "    filtered_lat_corpus[categorie] = {fichier: phrases for fichier, phrases in split_latin_corpus[categorie].items() if any(substring in fichier for substring in lat_target_authors)}\n",
        "for categorie in split_greek_corpus:\n",
        "    filtered_grc_corpus[categorie] = {fichier: phrases for fichier, phrases in split_greek_corpus[categorie].items() if any(substring in fichier for substring in grc_target_authors)}"
      ],
      "metadata": {
        "id": "ff8NOOImpn_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokénisation avec le modèle BERT, en conservant les indices des subtokens des formes associées aux lemmes"
      ],
      "metadata": {
        "id": "hpTJ9gOHL4cv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtBH4EsSLlq0"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "def tokenize_batch(args):\n",
        "    batch_sentences_forms, batch_sentences_lemmas, tokenizer = args\n",
        "    # Convertir les listes de mots en phrases pour le tokénisateur\n",
        "    batch_sentences = [\" \".join(sentence) for sentence in batch_sentences_forms]\n",
        "\n",
        "    encoding = tokenizer(batch_sentences, truncation=True, padding='longest', return_tensors=\"pt\", max_length=512)\n",
        "\n",
        "    batch_tokens = []\n",
        "    batch_origin_indices = []\n",
        "    batch_subtoken_lemmas = []\n",
        "\n",
        "    for sent_forms, sent_lemmas in zip(batch_sentences_forms, batch_sentences_lemmas):\n",
        "        tokens = []\n",
        "        origin_indices = []\n",
        "        subtoken_lemmas = []\n",
        "        for idx, (form, lemma) in enumerate(zip(sent_forms, sent_lemmas)):\n",
        "            word_tokens = tokenizer.tokenize(form)\n",
        "            tokens.extend(word_tokens)\n",
        "            origin_indices.extend([idx] * len(word_tokens))\n",
        "            subtoken_lemmas.extend([lemma] * len(word_tokens))\n",
        "        batch_tokens.append(tokens)\n",
        "        batch_origin_indices.append(origin_indices)\n",
        "        batch_subtoken_lemmas.append(subtoken_lemmas)\n",
        "\n",
        "    return encoding[\"input_ids\"].tolist(), encoding[\"attention_mask\"].tolist(), batch_sentences, batch_tokens, batch_origin_indices, batch_subtoken_lemmas\n",
        "\n",
        "def batched_tokenization(corpus, tokenizer, batch_size):\n",
        "    all_input_ids = []\n",
        "    all_attention_masks = []\n",
        "    all_sentence_tokens = []\n",
        "    all_origin_indices = []\n",
        "    all_subtoken_lemmas = []\n",
        "\n",
        "    # Génération d'une liste de toutes les phrases du corpus pour les formes et les lemmes\n",
        "    sentences_forms = [sentence for text in corpus[\"forms\"].values() for sentence in text]\n",
        "    sentences_lemmas = [sentence for text in corpus[\"lemmas\"].values() for sentence in text]\n",
        "\n",
        "    assert len(sentences_forms) == len(sentences_lemmas),\n",
        "\n",
        "    # Préparation des batches de phrases\n",
        "    batches = [(sentences_forms[i:i+batch_size], sentences_lemmas[i:i+batch_size], tokenizer) for i in range(0, len(sentences_forms), batch_size)]\n",
        "\n",
        "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
        "        for input_ids, attention_masks, batch_sents, batch_tokens, batch_origin_inds, batch_lemmas in tqdm(pool.imap_unordered(func=tokenize_batch, iterable=batches), total=len(batches), desc=\"Tokenizing sentences\"):\n",
        "            all_input_ids.extend(input_ids)\n",
        "            all_attention_masks.extend(attention_masks)\n",
        "            all_sentence_tokens.extend(batch_tokens)\n",
        "            all_origin_indices.extend(batch_origin_inds)\n",
        "            all_subtoken_lemmas.extend(batch_lemmas)\n",
        "\n",
        "    return all_input_ids, all_attention_masks, all_sentence_tokens, all_origin_indices, all_subtoken_lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4IDgJufd4yX"
      },
      "outputs": [],
      "source": [
        "greek_all_input_ids, greek_all_attention_masks, greek_all_sentences_tokens, greek_all_origin_indices, greek_all_subtoken_lemmas = batched_tokenization(filtered_grc_corpus, tokenizer, batch_size=512)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latin_all_input_ids, latin_all_attention_masks, latin_all_sentences_tokens, latin_all_origin_indices, latin_all_subtoken_lemmas = batched_tokenization(filtered_lat_corpus, tokenizer, batch_size=512)"
      ],
      "metadata": {
        "id": "Hd7ZYCUqsAFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérification de la tokénisation"
      ],
      "metadata": {
        "id": "9qv4itrSMEvE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84AWweJuesEf",
        "outputId": "1c249b77-2167-4259-cc1e-f7ffdb6c118e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input IDs: [[101, 479, 10781, 12649, 58317, 468, 15233, 17198, 31712, 480, 13140, 17762, 484, 31712, 472, 21263, 31712, 10649, 465, 14669, 29613, 467, 99509, 23788, 12649, 465, 31625, 53428, 19491, 44306, 10484, 483, 14669, 87276, 58281, 12649, 469, 29223, 12526, 10487, 480, 19582, 29223, 14669, 27393, 19038, 12649, 10356, 475, 15233, 70076, 15860, 117, 468, 12526, 110568, 16099, 479, 14669, 63444, 485, 34359, 16146, 35790, 51650, 480, 22360, 15751, 484, 31712, 465, 14669, 70076, 117, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 10487, 484, 21263, 465, 43140, 62913, 33947, 10358, 10487, 474, 13140, 20660, 12649, 10358, 10487, 471, 84236, 14669, 117, 480, 60846, 15233, 27835, 10484, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 474, 13140, 17198, 62152, 87728, 10487, 468, 12526, 24767, 19038, 15751, 91452, 117, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 475, 31712, 87728, 468, 111010, 485, 106619, 50379, 43202, 217, 117, 480, 22360, 39763, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 471, 84236, 12526, 87728, 468, 111010, 479, 14669, 12850, 469, 17762, 10487, 484, 14669, 83494, 12649, 217, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Sample attention masks: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "Sample sentence tokens: [['ο', 'με', '##ν', 'εν', 'δ', '##η', '##λ', '##ω', 'π', '##α', '##ρα', 'τ', '##ω', 'θ', '##ε', '##ω', 'την', 'α', '##υ', '##του', 'γ', '##νω', '##μη', '##ν', 'α', '##πο', '##φη', '##να', '##μενο', '##ς', 'σ', '##υ', '##νε', '##γραψε', '##ν', 'ε', '##π', '##ι', 'το', 'π', '##ρο', '##π', '##υ', '##λα', '##ιο', '##ν', 'του', 'λ', '##η', '##τω', '##ου', ',', 'δ', '##ι', '##ελ', '##ων', 'ο', '##υ', '##χ', 'υ', '##πα', '##ρ', '##χο', '##ντα', 'π', '##αν', '##τα', 'τ', '##ω', 'α', '##υ', '##τω', ','], ['το', 'τ', '##ε', 'α', '##γα', '##θ', '##ον', 'και', 'το', 'κ', '##α', '##λο', '##ν', 'και', 'το', 'η', '##δ', '##υ', ',', 'π', '##οι', '##η', '##σα', '##ς'], ['κ', '##α', '##λ', '##λι', '##στον', 'το', 'δ', '##ι', '##κα', '##ιο', '##τα', '##τον', ','], ['λ', '##ω', '##στον', 'δ', '##ʼ', 'υ', '##για', '##ινε', '##ιν', '·', ',', 'π', '##αν', '##των'], ['η', '##δ', '##ι', '##στον', 'δ', '##ʼ', 'ο', '##υ', 'τις', 'ε', '##ρα', 'το', 'τ', '##υ', '##χει', '##ν', '·']]\n",
            "Sample sentence tokens: [['ὁ', 'μέν', 'μέν', 'ἐν', 'Δῆλος', 'Δῆλος', 'Δῆλος', 'Δῆλος', 'παρά', 'παρά', 'παρά', 'ὁ', 'ὁ', 'θεός', 'θεός', 'θεός', 'ὁ', 'αὐτός', 'αὐτός', 'αὐτός', 'γνώμη', 'γνώμη', 'γνώμη', 'γνώμη', 'ἀποφήνω', 'ἀποφήνω', 'ἀποφήνω', 'ἀποφήνω', 'ἀποφήνω', 'ἀποφήνω', 'συγγράφω', 'συγγράφω', 'συγγράφω', 'συγγράφω', 'συγγράφω', 'ἐπί', 'ἐπί', 'ἐπί', 'ὁ', 'προπύλαιος', 'προπύλαιος', 'προπύλαιος', 'προπύλαιος', 'προπύλαιος', 'προπύλαιος', 'προπύλαιος', 'ὁ', 'Λητόος', 'Λητόος', 'Λητόος', 'Λητόος', 'Λητόος', 'διαιρέω', 'διαιρέω', 'διαιρέω', 'διαιρέω', 'οὐ', 'οὐ', 'οὐ', 'ὑπάρχω', 'ὑπάρχω', 'ὑπάρχω', 'ὑπάρχω', 'ὑπάρχω', 'πᾶς', 'πᾶς', 'πᾶς', 'ὁ', 'ὁ', 'αὐτός', 'αὐτός', 'αὐτός', 'αὐτός'], ['ὁ', 'τε', 'τε', 'ἀγαθός', 'ἀγαθός', 'ἀγαθός', 'ἀγαθός', 'καί', 'ὁ', 'καλός', 'καλός', 'καλός', 'καλός', 'καί', 'ὁ', 'ἡδύ,', 'ἡδύ,', 'ἡδύ,', 'ἡδύ,', 'ποιέω', 'ποιέω', 'ποιέω', 'ποιέω', 'ποιέω'], ['καλός', 'καλός', 'καλός', 'καλός', 'καλός', 'ὁ', 'δίκαιος', 'δίκαιος', 'δίκαιος', 'δίκαιος', 'δίκαιος', 'δίκαιος', 'δίκαιος'], ['λῷστος', 'λῷστος', 'λῷστος', 'δή', 'δή', 'ὑγιαίνω', 'ὑγιαίνω', 'ὑγιαίνω', 'ὑγιαίνω', 'ὑγιαίνω', 'ὑγιαίνω', 'πᾶς', 'πᾶς', 'πᾶς'], ['ἥδιστος', 'ἥδιστος', 'ἥδιστος', 'ἥδιστος', 'δή', 'δή', 'ὅς', 'ὅς', 'τὶς', 'λέγω', 'λέγω', 'ὁ', 'τυγχάνω', 'τυγχάνω', 'τυγχάνω', 'τυγχάνω', 'τυγχάνω']]\n"
          ]
        }
      ],
      "source": [
        "print(\"Sample input IDs:\", greek_all_input_ids[:5])\n",
        "print(\"Sample attention masks:\", greek_all_attention_masks[:5])\n",
        "print(\"Sample sentence tokens:\", greek_all_sentences_tokens[:5])\n",
        "print(\"Sample sentence tokens:\", greek_all_subtoken_lemmas[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sauvegarde"
      ],
      "metadata": {
        "id": "aMR0jkD9MIjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/embeddings_save/greek_all_input_ids_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(greek_all_input_ids, f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/greek_attention_masks_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(greek_all_attention_masks, f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/greek_all_sentences_tokens_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(greek_all_sentences_tokens, f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/greek_all_origin_indices_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(greek_all_origin_indices, f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/greek_all_subtoken_lemmas_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(greek_all_subtoken_lemmas, f)"
      ],
      "metadata": {
        "id": "0SD_Hvnxtd3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/embeddings_save/latin_all_input_ids_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(latin_all_input_ids, f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/latin_attention_masks_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(latin_all_attention_masks, f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/latin_all_sentences_tokens_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(latin_all_sentences_tokens, f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/latin_all_origin_indices_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(latin_all_origin_indices, f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/latin_all_subtoken_lemmas_512.pkl\", \"wb\") as f:\n",
        "    pickle.dump(latin_all_subtoken_lemmas, f)"
      ],
      "metadata": {
        "id": "X4tpp02Ktn2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/embeddings_save/greek_all_input_ids_512.pkl\", \"rb\") as f:\n",
        "    greek_all_input_ids = pickle.load(f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/greek_attention_masks_512.pkl\", \"rb\") as f:\n",
        "    greek_all_attention_masks = pickle.load(f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/greek_all_sentences_tokens_512.pkl\", \"rb\") as f:\n",
        "    greek_all_sentences_tokens = pickle.load(f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/greek_all_subtoken_lemmas_512.pkl\", \"rb\") as f:\n",
        "    greek_all_subtoken_lemmas = pickle.load(f)"
      ],
      "metadata": {
        "id": "peCx1esmuHH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/embeddings_save/latin_all_input_ids_512.pkl\", \"rb\") as f:\n",
        "    latin_all_input_ids = pickle.load(f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/latin_attention_masks_512.pkl\", \"rb\") as f:\n",
        "    latin_all_attention_masks = pickle.load(f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/latin_all_sentences_tokens_512.pkl\", \"rb\") as f:\n",
        "    latin_all_sentences_tokens = pickle.load(f)\n",
        "with open(\"/content/drive/MyDrive/embeddings_save/latin_all_subtoken_lemmas_512.pkl\", \"rb\") as f:\n",
        "    latin_all_subtoken_lemmas = pickle.load(f)"
      ],
      "metadata": {
        "id": "AVUhBlZ_uqx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcul des vecteurs"
      ],
      "metadata": {
        "id": "9uSKWBnbMarA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fonction qui permet d'impacter le moyennage des subtokens en un seul token : la partie du milieu d'un mot a plus ou moins de poids sur l'ensemble du vecteur.\n",
        "<br>Pour faire sans cette fonction, il faut appeler la fonction `get_contextual_embeddings_without_weights`"
      ],
      "metadata": {
        "id": "swLTS9iyMf9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlOrEIA3DKkc"
      },
      "outputs": [],
      "source": [
        "def compute_weights(length):\n",
        "    # Création d'un array avec les positions des tokens\n",
        "    positions = np.arange(length)\n",
        "\n",
        "    # Calcul de la position du milieu\n",
        "    mid_position = length / 2.0\n",
        "\n",
        "    # Paramètre de mise à l'échelle pour contrôler la largeur de la fonction gaussienne\n",
        "    sigma = length / 6.0\n",
        "\n",
        "    # Calcul des poids en utilisant une fonction gaussienne\n",
        "    weights = np.exp(-(positions - mid_position)**2 / (2 * sigma**2))\n",
        "\n",
        "    # Normalisation des poids pour qu'ils somment à 1\n",
        "    weights /= weights.sum()\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuTw7_M3LMyV"
      },
      "outputs": [],
      "source": [
        "def get_contextual_embeddings_without_weights(processed_texts, model, tokenizer, device, all_input_ids, all_attention_masks, all_origin_indices, all_sentences_tokens, all_subtoken_lemmas, batch_size=512):\n",
        "    model.eval()\n",
        "    lemma_embeddings_sum = {}\n",
        "    lemma_token_counts = {}\n",
        "\n",
        "    current_idx = 0\n",
        "    for filename, forms_sentences in tqdm(processed_texts[\"forms\"].items(), desc=\"Processing files\"):\n",
        "        lemmas_sentences = processed_texts[\"lemmas\"][filename]\n",
        "\n",
        "        for forms_sentence, lemmas_sentence in zip(forms_sentences, lemmas_sentences):\n",
        "            assert len(forms_sentence) == len(lemmas_sentence), f\"Forms and lemmas length mismatch in file {filename}.\"\n",
        "\n",
        "            input_ids = all_input_ids[current_idx]\n",
        "            attention_masks = all_attention_masks[current_idx]\n",
        "            origin_indices = all_origin_indices[current_idx]\n",
        "            subtoken_lemmas = all_subtoken_lemmas[current_idx]  # Lemmas pour les subtokens actuels\n",
        "\n",
        "            input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "            attention_masks_tensor = torch.tensor(attention_masks).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                  outputs = model(input_tensor, attention_mask=attention_masks_tensor)\n",
        "                  hidden_states = outputs.last_hidden_state[0]\n",
        "\n",
        "            # Pas de logique de poids ici\n",
        "\n",
        "            for token_idx, (token, lemma) in enumerate(zip(all_sentences_tokens[current_idx], subtoken_lemmas)):\n",
        "                if lemma not in lemma_embeddings_sum:\n",
        "                    lemma_embeddings_sum[lemma] = np.zeros(hidden_states.shape[1], dtype=np.float32)\n",
        "                    lemma_token_counts[lemma] = 0\n",
        "\n",
        "                lemma_embeddings_sum[lemma] += hidden_states[token_idx].cpu().numpy()  # Simplement ajouter l'embedding\n",
        "                lemma_token_counts[lemma] += 1\n",
        "\n",
        "            current_idx += 1\n",
        "\n",
        "    averaged_embeddings = {lemma: lemma_embeddings_sum[lemma] / lemma_token_counts[lemma] for lemma in lemma_embeddings_sum.keys()}\n",
        "    return averaged_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ5d7oa-DzM7"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def get_contextual_embeddings_with_weights(processed_texts, model, tokenizer, device, all_input_ids, all_attention_masks, all_origin_indices, all_sentences_tokens, all_subtoken_lemmas, batch_size=256):\n",
        "    model.eval()\n",
        "    lemma_embeddings_sum = {}\n",
        "    lemma_token_counts = {}\n",
        "\n",
        "    # Total number of sentences\n",
        "    total_sentences = len(all_input_ids)\n",
        "\n",
        "    for start_idx in tqdm(range(0, total_sentences, batch_size), desc=\"Processing batches\"):\n",
        "        end_idx = min(start_idx + batch_size, total_sentences)\n",
        "\n",
        "        # Extract batched data\n",
        "        batch_input_ids = all_input_ids[start_idx:end_idx]\n",
        "        batch_attention_masks = all_attention_masks[start_idx:end_idx]\n",
        "        batch_sentences_tokens = all_sentences_tokens[start_idx:end_idx]\n",
        "        batch_subtoken_lemmas = all_subtoken_lemmas[start_idx:end_idx]\n",
        "\n",
        "        # Padding sequences within the batch to have the same length\n",
        "        input_tensor = pad_sequence([torch.tensor(seq) for seq in batch_input_ids], batch_first=True).to(device)\n",
        "        attention_masks_tensor = pad_sequence([torch.tensor(mask) for mask in batch_attention_masks], batch_first=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_tensor, attention_mask=attention_masks_tensor)\n",
        "            batch_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        # Iterate over individual sentence embeddings in the batch\n",
        "        for idx, hidden_states in enumerate(batch_hidden_states):\n",
        "            weights = compute_weights(len(batch_sentences_tokens[idx]))\n",
        "\n",
        "            for token_idx, (token, lemma) in enumerate(zip(batch_sentences_tokens[idx], batch_subtoken_lemmas[idx])):\n",
        "                if lemma not in lemma_embeddings_sum:\n",
        "                    lemma_embeddings_sum[lemma] = np.zeros(hidden_states.shape[1], dtype=np.float32)\n",
        "                    lemma_token_counts[lemma] = 0\n",
        "\n",
        "                lemma_embeddings_sum[lemma] += weights[token_idx] * hidden_states[token_idx].cpu().numpy()\n",
        "                lemma_token_counts[lemma] += weights[token_idx]\n",
        "\n",
        "    averaged_embeddings = {lemma: lemma_embeddings_sum[lemma] / lemma_token_counts[lemma] for lemma in lemma_embeddings_sum.keys()}\n",
        "    return averaged_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qSWD_DczjL5"
      },
      "outputs": [],
      "source": [
        "philo_greek_embeddings = get_contextual_embeddings_with_weights(\n",
        "    processed_texts=filtered_grc_corpus,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    all_input_ids=greek_all_input_ids,\n",
        "    all_attention_masks=greek_all_attention_masks,\n",
        "    all_sentences_tokens = greek_all_sentences_tokens,\n",
        "    all_origin_indices=greek_all_origin_indices,\n",
        "    all_subtoken_lemmas=greek_all_subtoken_lemmas\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "philo_latin_embeddings = get_contextual_embeddings_with_weights(\n",
        "    processed_texts=filtered_lat_corpus,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    all_input_ids=latin_all_input_ids,\n",
        "    all_attention_masks=latin_all_attention_masks,\n",
        "    all_sentences_tokens = latin_all_sentences_tokens,\n",
        "    all_origin_indices=latin_all_origin_indices,\n",
        "    all_subtoken_lemmas=latin_all_subtoken_lemmas\n",
        ")"
      ],
      "metadata": {
        "id": "0rSuKU-usuVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vérification"
      ],
      "metadata": {
        "id": "-5chXVbPNPLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random_lemmas = random.sample(list(philo_greek_embeddings.keys()), 10)\n",
        "for lemma in random_lemmas:\n",
        "    print(lemma, philo_greek_embeddings[lemma][:5])"
      ],
      "metadata": {
        "id": "g4ypo4IRQRmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sauvegarde"
      ],
      "metadata": {
        "id": "V_8prlCfNRS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hr2uwGgh5TL"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/embeddings_save/philo_greek_embeddings.pkl', 'wb') as file:\n",
        "    pickle.dump(philo_greek_embeddings, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/embeddings_save/philo_latin_embeddings.pkl', 'wb') as file:\n",
        "    pickle.dump(philo_latin_embeddings, file)"
      ],
      "metadata": {
        "id": "2-wt_Uv-6hma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocessing sur les embeddings"
      ],
      "metadata": {
        "id": "h8DPF48RNU65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Téléchargement des stopwords. Il s'agit ici d'un dépôt public, mais les listes sont celles proposées par CLTK."
      ],
      "metadata": {
        "id": "wnISPz8wNbRY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgg-yawf8bvY"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1MZ4ld8j30ye3YGYy-T7V3Cyy1c4dlGks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fvLw_s3qaTk"
      },
      "outputs": [],
      "source": [
        "!gdown --id 161g7Kdv4PCFp2iYJAlMbHkb-CGstsNcu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA2jY4CJHhg2"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/stopwords_gk.txt\", encoding = \"utf8\") as stop_file:\n",
        "  stopwords_diacritics=stop_file.read().split(\"\\n\")\n",
        "stopwords_greek=set(stopwords_diacritics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/stopwords_lat.txt\", encoding = \"utf8\") as stop_file:\n",
        "  stopwords_diacritics=stop_file.read().split(\"\\n\")\n",
        "stopwords_latin=set(stopwords_diacritics)"
      ],
      "metadata": {
        "id": "njVPZ6eb8xIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppression des embeddings de mots outils"
      ],
      "metadata": {
        "id": "svR5dfMnNtdk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acNnU_oA80rf"
      },
      "outputs": [],
      "source": [
        "no_stop_greek_embeddings = {k: v for k, v in philo_greek_embeddings.items() if k not in stopwords_greek}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no_stop_latin_embeddings = {k: v for k, v in philo_latin_embeddings.items() if k not in stopwords_latin}"
      ],
      "metadata": {
        "id": "veN7K-WS8__0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conservation des embeddings des 10000 mots les plus fréquents (pour la représention graphique --> n'impacte pas le calcul)."
      ],
      "metadata": {
        "id": "yevYVicXN0Qo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iog9TXSrvjAA"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 1. Calculer la fréquence des lemmes\n",
        "frequency_counter = Counter()\n",
        "for filename, lemmatized_sentences in split_greek_corpus[\"lemmas\"].items():  # Attention ici, j'ai ajouté [\"lemmas\"]\n",
        "    for lemmatized_text in lemmatized_sentences:\n",
        "        for lemma in lemmatized_text:\n",
        "            frequency_counter[lemma] += 1\n",
        "\n",
        "# 2. Trier les lemmes par fréquence\n",
        "sorted_lemmas = [item[0] for item in frequency_counter.most_common()]\n",
        "\n",
        "# 3. Sélectionner les top N lemmes\n",
        "top_greek_lemmas = set(sorted_lemmas[:10000])\n",
        "\n",
        "# 4. Filtrer les embeddings\n",
        "greek_filtered_embeddings = {lemma: no_stop_greek_embeddings[lemma] for lemma in top_greek_lemmas if lemma in no_stop_greek_embeddings}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Calculer la fréquence des lemmes\n",
        "frequency_counter = Counter()\n",
        "for filename, lemmatized_sentences in split_latin_corpus[\"lemmas\"].items():  # Attention ici, j'ai ajouté [\"lemmas\"]\n",
        "    for lemmatized_text in lemmatized_sentences:\n",
        "        for lemma in lemmatized_text:\n",
        "            frequency_counter[lemma] += 1\n",
        "\n",
        "# 2. Trier les lemmes par fréquence\n",
        "sorted_lemmas = [item[0] for item in frequency_counter.most_common()]\n",
        "\n",
        "# 3. Sélectionner les top N lemmes\n",
        "top_latin_lemmas = set(sorted_lemmas[:10000])\n",
        "\n",
        "# 4. Filtrer les embeddings\n",
        "latin_filtered_embeddings = {lemma: no_stop_latin_embeddings[lemma] for lemma in top_latin_lemmas if lemma in no_stop_latin_embeddings}"
      ],
      "metadata": {
        "id": "qbIPByht9caS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sauvegarde dans un fichier de vecteurs compatible tensorflow (embeddings projector)"
      ],
      "metadata": {
        "id": "9SkM4hp8ORmB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5SQbzaT4-VN"
      },
      "outputs": [],
      "source": [
        "# Extract the lemmas and their embeddings\n",
        "all_embeddings = {**latin_filtered_embeddings, **greek_filtered_embeddings}\n",
        "# Save embeddings to the vectors file\n",
        "with open('vectors.tsv', 'w') as f_vectors:\n",
        "    for lemma, embedding in all_embeddings.items():\n",
        "        f_vectors.write('\\t'.join([str(value) for value in embedding]) + '\\n')\n",
        "\n",
        "with open('metadata.tsv', 'w') as f_metadata:\n",
        "    for lemma in all_embeddings.keys():\n",
        "        f_metadata.write(lemma + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calcul de la similarité avec similarité cosinus"
      ],
      "metadata": {
        "id": "nKGV1PGLOuKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = np.zeros((len(reduced_latin_dict), len(reduced_greek_dict)))\n",
        "\n",
        "def find_most_similar(target_embedding, embeddings_dict):\n",
        "    similarities = {}\n",
        "    for word, embedding in embeddings_dict.items():\n",
        "        sim = cosine_similarity([target_embedding], [embedding])[0][0]\n",
        "        similarities[word] = sim\n",
        "\n",
        "    sorted_items = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return sorted_items[:10]"
      ],
      "metadata": {
        "id": "4Vf7-zbaLlqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greek_word = \"δόξα\"\n",
        "greek_embedding = reduced_greek_dict[greek_word]\n",
        "\n",
        "top_latin_words_with_scores = find_most_similar(greek_embedding, reduced_greek_dict)\n",
        "\n",
        "for word, score in top_latin_words_with_scores:\n",
        "    print(f\"{word}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "RGjxfn9XNIQ5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORqd4zIRy4eWhlSbxE1UdJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}